{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "download_imagenet_using_python.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7585ObXuu1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mounting Google Drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uTvsbKcuzUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd gdrive/My\\ Drive/imagenet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jjFkacnu5IK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import urllib.parse\n",
        "import numpy as np\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkHxYDsLu7Ai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "page = requests.get('http://image-net.org/challenges/LSVRC/2012/browse-synsets')\n",
        "soup = BeautifulSoup(page.content, 'html.parser')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_pqPS0yu8fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Collecting synset ids \n",
        "wnids = []\n",
        "for link in soup.find_all('a'):\n",
        "  if 'wnid' in link.get('href'):\n",
        "    url = urllib.parse.urlparse(link.get('href'))\n",
        "    params = urllib.parse.parse_qs(url.query)\n",
        "    if 'wnid' in params:\n",
        "      wnids.append([params['wnid'][0], link.get_text()])\n",
        "print(len(wnids))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLbT-0H3u-pd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For checking if download is being resumed when the notebook is run again\n",
        "import os\n",
        "resume = False\n",
        "if os.path.isdir(\"./train\") == True and os.path.isdir(\"./validation\") == True:\n",
        "  resume = True\n",
        "else:\n",
        "  !mkdir ./train \n",
        "  !mkdir ./validation\n",
        "\n",
        "print(resume)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zekhhbpvB-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finding out which synset/directory to resume downloading from\n",
        "resume_index = None\n",
        "\n",
        "if resume == True:\n",
        "  last_folder_index = !ls -1t ./train/ | wc -l\n",
        "  last_folder_index = int(last_folder_index[0])\n",
        "  \n",
        "  last_folder_name = wnids[last_folder_index+1][1].split(',')[0].replace(' ', '_').replace(\"'\", \"\")\n",
        "  check = !ls -1t ./train/$last_folder_name | wc -l\n",
        "  print(last_folder_name)\n",
        "  if int(check[0]) == 101:\n",
        "    resume_index = last_folder_index + 2\n",
        "  else:\n",
        "    !rm -rf ./train/$last_folder_name\n",
        "    !mkdir ./train/$last_folder_name\n",
        "    resume_index = last_folder_index\n",
        "\n",
        "  check = !ls -1t ./train/$last_folder_name | wc -l\n",
        "  print(check[0])\n",
        "  \n",
        "print(wnids[resume_index])\n",
        "print(resume_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdDEg7GBvGcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def url_to_image(url):\n",
        "  '''\n",
        "  download image, convert to NumPy array and return in OpenCV format\n",
        "  '''\n",
        "  try:\n",
        "    resp = urllib.request.urlopen(url, timeout=20)\n",
        "  except Exception as e:\n",
        "    return\n",
        "  \n",
        "  try:\n",
        "    resp.getcode()\n",
        "    image = np.asarray(bytearray(resp.read()), dtype='uint8')\n",
        "  except Exception as e:\n",
        "    print(str(e))\n",
        "    return\n",
        "  image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "  return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW18peHDvLzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_images(urls, label):\n",
        "  count = 0\n",
        "\n",
        "  for i, url in enumerate(urls):\n",
        "    try:\n",
        "      I = url_to_image(url)\n",
        "    except Exception as e:\n",
        "      print(str(e))\n",
        "    \n",
        "    if I is not None:\n",
        "      save_path = './train/' + label + '/img' + str(i) + '.jpg'\n",
        "      print(save_path)\n",
        "      result = cv2.imwrite(save_path, I)\n",
        "      if not result:\n",
        "        print('Could not write image.')\n",
        "      count += 1\n",
        "      if count > 100:\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV1hlEnRvOTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Images downloaded would initially be present in train folder\n",
        "template = \"http://image-net.org/api/text/imagenet.synset.geturls?wnid=\"\n",
        "train_dir = \"./train/\"\n",
        "validation_dir = \"./validation/\"\n",
        "\n",
        "for uid, label in wnids[resume_index:]:\n",
        "  page_url = template+str(uid)\n",
        "  print(page_url, label)\n",
        "  page = requests.get(page_url)\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "  str_soup = str(soup)\n",
        "  split_urls = str_soup.split('\\r\\n')\n",
        "\n",
        "  l = label.split(',')[0].replace(' ', '_').replace(\"'\", \"\")\n",
        "  t_dir = train_dir + l\n",
        "  v_dir = validation_dir + l\n",
        "  print(t_dir)\n",
        "  !mkdir {t_dir}\n",
        "  !mkdir {v_dir}\n",
        "\n",
        "  download_images(split_urls, l)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}